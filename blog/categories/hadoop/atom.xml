<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hadoop | Stack Dump]]></title>
  <link href="http://hhimanshu.github.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://hhimanshu.github.com/"/>
  <updated>2012-08-03T11:03:14-07:00</updated>
  <id>http://hhimanshu.github.com/</id>
  <author>
    <name><![CDATA[Harit Himanshu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Basic Template for your most Map Reduce programs]]></title>
    <link href="http://hhimanshu.github.com/blog/2012/08/02/basic-template-for-your-most-map-reduce-programs/"/>
    <updated>2012-08-02T15:38:00-07:00</updated>
    <id>http://hhimanshu.github.com/blog/2012/08/02/basic-template-for-your-most-map-reduce-programs</id>
    <content type="html"><![CDATA[<p>This is true that you would never have to write a Map Reduce program from scratch. This is what I learned while reading <a href="http://manning.com/lam/">Hadoop in Action</a></p>

<p>I thought I would be a nice thing to write a basic Map Reduce skeleton that I and almost anyone want to write Map Reduce program can leverage.</p>

<p>Here is it</p>

<p><div><script src='https://gist.github.com/3241462.js?file='></script>
<noscript><pre><code>import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import java.io.IOException;

public class MapReduceClass extends Configured implements Tool {

    public static class MapClass extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; {

        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            // your map code goes here

        }
    }


    public static class Reduce extends Reducer&lt;Text, LongWritable, Text, Text&gt; {

        public void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException {
            // your reduce function goes here
    }

    public int run(String args[]) throws Exception {
        Job job = new Job();
        job.setJarByClass(MapReduceClass.class);

        job.setMapperClass(MapClass.class);
        job.setReducerClass(Reduce.class);

        FileInputFormat.setInputPaths(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        // if your map output key/value classes are different than your input key/value classes
        // see - http://stackoverflow.com/questions/11761135/hadoop-job-fails-while-reducing-java-io-ioexception-type-mismatch-in-value-fro
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(LongWritable.class);
        
        job.setJobName(&quot;MapReduceClass&quot;);

        // uncomment the following if you just want to run Mapper and not Reducer
//        job.setNumReduceTasks(0);
        boolean success = job.waitForCompletion(true);
        return success ? 0 : 1;
    }

    public static void main(String args[]) throws Exception {
        int ret = ToolRunner.run(new MapReduceClass(), args);
        System.exit(ret);
    }
}</code></pre></noscript></div>
</p>

<p>Let me know if you see any issues with that or you would like to share anything that would be useful.</p>
]]></content>
  </entry>
  
</feed>
